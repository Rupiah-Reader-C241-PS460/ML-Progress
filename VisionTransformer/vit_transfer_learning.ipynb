{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995f8623392be588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:23:55.807461Z",
     "start_time": "2024-06-12T12:23:52.487416Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import create_optimizer\n",
    "from transformers import TFAutoModelForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7897a1a303bb1737",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:00.293858Z",
     "start_time": "2024-06-12T12:24:00.216996Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True) #limits gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T07:18:11.649216Z",
     "start_time": "2024-06-12T06:30:21.010140Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n",
      "From (original): https://drive.google.com/uc?id=14m2XW31x_UWAqUeoM0SNVsH3kwyBIAVy\r\n",
      "From (redirected): https://drive.google.com/uc?id=14m2XW31x_UWAqUeoM0SNVsH3kwyBIAVy&confirm=t&uuid=743331cf-719b-4f07-bc73-d29eb1164af5\r\n",
      "To: /home/remunata/dev/bangkit/ML-Progress/Vision Transformer (ViT)/data.zip\r\n",
      "100%|███████████████████████████████████████| 2.48G/2.48G [47:36<00:00, 870kB/s]\r\n"
     ]
    }
   ],
   "source": [
    "!gdown 14m2XW31x_UWAqUeoM0SNVsH3kwyBIAVy\n",
    "\n",
    "zip_ref = zipfile.ZipFile('data.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7966f399db61dbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:13.709644Z",
     "start_time": "2024-06-12T12:24:05.097081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427aaacccf984a0b9666ef16b2ca977f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40129364deff4f518acaf05b2c3c7277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f69b7f22d2648dfbfb3cfcf85b4e502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir='data')\n",
    "\n",
    "labels = dataset['train'].features['label'].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48fad453c7b95a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:14.738575Z",
     "start_time": "2024-06-12T12:24:14.723687Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6171871a434727d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:18.914411Z",
     "start_time": "2024-06-12T12:24:18.780458Z"
    }
   },
   "outputs": [],
   "source": [
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    "train_data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.RandomCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 255.0),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")\n",
    "\n",
    "val_data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.CenterCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 255.0),\n",
    "    ],\n",
    "    name=\"val_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b565fb2597aea54e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:19.359924Z",
     "start_time": "2024-06-12T12:24:19.349678Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_tf_tensor(image: Image):\n",
    "    np_image = np.array(image)\n",
    "    tf_image = tf.convert_to_tensor(np_image)\n",
    "    return tf.expand_dims(tf_image, 0)\n",
    "\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    images = [\n",
    "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    images = [\n",
    "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79033b17cad43014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:21.723338Z",
     "start_time": "2024-06-12T12:24:21.659301Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"].set_transform(preprocess_train)\n",
    "dataset[\"validation\"].set_transform(preprocess_val)\n",
    "dataset[\"test\"].set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166734ec204f0f6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:22.226120Z",
     "start_time": "2024-06-12T12:24:22.216805Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "num_train_steps = len(dataset[\"train\"]) * num_epochs\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced3c94b9eb4ab9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:27.136502Z",
     "start_time": "2024-06-12T12:24:23.238585Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = dataset[\"train\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_eval_dataset = dataset[\"validation\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_test_dataset = dataset[\"test\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8530b99abbbf8ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:29.293461Z",
     "start_time": "2024-06-12T12:24:28.313740Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFViTForImageClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e91490ccfa95a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:31.112063Z",
     "start_time": "2024-06-12T12:24:31.087325Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d99d3b812dae0902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:53:50.631966Z",
     "start_time": "2024-06-12T12:24:31.329704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7e3fb579da80> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x7e3fb579da80> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "832/832 [==============================] - 1018s 1s/step - loss: 0.7981 - accuracy: 0.7851 - val_loss: 0.4905 - val_accuracy: 0.8652\n",
      "Epoch 2/50\n",
      "832/832 [==============================] - 940s 1s/step - loss: 0.3338 - accuracy: 0.9030 - val_loss: 0.6146 - val_accuracy: 0.8026\n",
      "Epoch 3/50\n",
      "832/832 [==============================] - 942s 1s/step - loss: 0.2704 - accuracy: 0.9116 - val_loss: 0.3787 - val_accuracy: 0.8946\n",
      "Epoch 4/50\n",
      "832/832 [==============================] - 938s 1s/step - loss: 0.2340 - accuracy: 0.9232 - val_loss: 0.4146 - val_accuracy: 0.8753\n",
      "Epoch 5/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.2140 - accuracy: 0.9275 - val_loss: 0.3435 - val_accuracy: 0.9069\n",
      "Epoch 6/50\n",
      "832/832 [==============================] - 938s 1s/step - loss: 0.1954 - accuracy: 0.9295 - val_loss: 0.3004 - val_accuracy: 0.9256\n",
      "Epoch 7/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1875 - accuracy: 0.9353 - val_loss: 0.3683 - val_accuracy: 0.9032\n",
      "Epoch 8/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.1693 - accuracy: 0.9424 - val_loss: 0.3129 - val_accuracy: 0.9160\n",
      "Epoch 9/50\n",
      "832/832 [==============================] - 934s 1s/step - loss: 0.1710 - accuracy: 0.9395 - val_loss: 0.4074 - val_accuracy: 0.9005\n",
      "Epoch 10/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1708 - accuracy: 0.9388 - val_loss: 0.3472 - val_accuracy: 0.8850\n",
      "Epoch 11/50\n",
      "832/832 [==============================] - 939s 1s/step - loss: 0.1612 - accuracy: 0.9398 - val_loss: 0.2890 - val_accuracy: 0.9246\n",
      "Epoch 12/50\n",
      "832/832 [==============================] - 939s 1s/step - loss: 0.1416 - accuracy: 0.9481 - val_loss: 0.3430 - val_accuracy: 0.9144\n",
      "Epoch 13/50\n",
      "832/832 [==============================] - 939s 1s/step - loss: 0.1468 - accuracy: 0.9451 - val_loss: 0.2320 - val_accuracy: 0.9246\n",
      "Epoch 14/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1539 - accuracy: 0.9454 - val_loss: 0.2174 - val_accuracy: 0.9374\n",
      "Epoch 15/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1394 - accuracy: 0.9499 - val_loss: 0.2080 - val_accuracy: 0.9374\n",
      "Epoch 16/50\n",
      "832/832 [==============================] - 939s 1s/step - loss: 0.1426 - accuracy: 0.9481 - val_loss: 0.2267 - val_accuracy: 0.9444\n",
      "Epoch 17/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1352 - accuracy: 0.9486 - val_loss: 0.2522 - val_accuracy: 0.9363\n",
      "Epoch 18/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1294 - accuracy: 0.9502 - val_loss: 0.3450 - val_accuracy: 0.9058\n",
      "Epoch 19/50\n",
      "832/832 [==============================] - 938s 1s/step - loss: 0.1374 - accuracy: 0.9478 - val_loss: 0.2574 - val_accuracy: 0.9320\n",
      "Epoch 20/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1237 - accuracy: 0.9550 - val_loss: 0.2626 - val_accuracy: 0.9411\n",
      "Epoch 21/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.1263 - accuracy: 0.9531 - val_loss: 0.2609 - val_accuracy: 0.9353\n",
      "Epoch 22/50\n",
      "832/832 [==============================] - 938s 1s/step - loss: 0.1288 - accuracy: 0.9529 - val_loss: 0.3821 - val_accuracy: 0.9128\n",
      "Epoch 23/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.1234 - accuracy: 0.9550 - val_loss: 0.2486 - val_accuracy: 0.9401\n",
      "Epoch 24/50\n",
      "832/832 [==============================] - 1010s 1s/step - loss: 0.1226 - accuracy: 0.9549 - val_loss: 0.2726 - val_accuracy: 0.9358\n",
      "Epoch 25/50\n",
      "832/832 [==============================] - 1062s 1s/step - loss: 0.1182 - accuracy: 0.9562 - val_loss: 0.2575 - val_accuracy: 0.9422\n",
      "Epoch 26/50\n",
      "832/832 [==============================] - 1002s 1s/step - loss: 0.1114 - accuracy: 0.9565 - val_loss: 0.3084 - val_accuracy: 0.9272\n",
      "Epoch 27/50\n",
      "832/832 [==============================] - 1002s 1s/step - loss: 0.1226 - accuracy: 0.9565 - val_loss: 0.1896 - val_accuracy: 0.9481\n",
      "Epoch 28/50\n",
      "832/832 [==============================] - 1002s 1s/step - loss: 0.1135 - accuracy: 0.9570 - val_loss: 0.2745 - val_accuracy: 0.9374\n",
      "Epoch 29/50\n",
      "832/832 [==============================] - 960s 1s/step - loss: 0.1205 - accuracy: 0.9568 - val_loss: 0.3325 - val_accuracy: 0.9213\n",
      "Epoch 30/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.1191 - accuracy: 0.9564 - val_loss: 0.2985 - val_accuracy: 0.9251\n",
      "Epoch 31/50\n",
      "832/832 [==============================] - 932s 1s/step - loss: 0.1123 - accuracy: 0.9570 - val_loss: 0.2018 - val_accuracy: 0.9374\n",
      "Epoch 32/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.1126 - accuracy: 0.9598 - val_loss: 0.2774 - val_accuracy: 0.9267\n",
      "Epoch 33/50\n",
      "832/832 [==============================] - 932s 1s/step - loss: 0.1132 - accuracy: 0.9573 - val_loss: 0.2883 - val_accuracy: 0.9401\n",
      "Epoch 34/50\n",
      "832/832 [==============================] - 933s 1s/step - loss: 0.1126 - accuracy: 0.9586 - val_loss: 0.2116 - val_accuracy: 0.9449\n",
      "Epoch 35/50\n",
      "832/832 [==============================] - 938s 1s/step - loss: 0.1079 - accuracy: 0.9603 - val_loss: 0.2740 - val_accuracy: 0.9390\n",
      "Epoch 36/50\n",
      "832/832 [==============================] - 939s 1s/step - loss: 0.1119 - accuracy: 0.9570 - val_loss: 0.3180 - val_accuracy: 0.9160\n",
      "Epoch 37/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1092 - accuracy: 0.9606 - val_loss: 0.2482 - val_accuracy: 0.9449\n",
      "Epoch 38/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.1111 - accuracy: 0.9579 - val_loss: 0.2452 - val_accuracy: 0.9358\n",
      "Epoch 39/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.1014 - accuracy: 0.9603 - val_loss: 0.1757 - val_accuracy: 0.9433\n",
      "Epoch 40/50\n",
      "832/832 [==============================] - 934s 1s/step - loss: 0.1032 - accuracy: 0.9594 - val_loss: 0.2447 - val_accuracy: 0.9513\n",
      "Epoch 41/50\n",
      "832/832 [==============================] - 934s 1s/step - loss: 0.1078 - accuracy: 0.9602 - val_loss: 0.1811 - val_accuracy: 0.9567\n",
      "Epoch 42/50\n",
      "832/832 [==============================] - 935s 1s/step - loss: 0.0916 - accuracy: 0.9653 - val_loss: 0.1988 - val_accuracy: 0.9518\n",
      "Epoch 43/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.0989 - accuracy: 0.9623 - val_loss: 0.3229 - val_accuracy: 0.9272\n",
      "Epoch 44/50\n",
      "832/832 [==============================] - 937s 1s/step - loss: 0.0924 - accuracy: 0.9642 - val_loss: 0.2284 - val_accuracy: 0.9422\n",
      "Epoch 45/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.1013 - accuracy: 0.9639 - val_loss: 0.1987 - val_accuracy: 0.9545\n",
      "Epoch 46/50\n",
      "832/832 [==============================] - 939s 1s/step - loss: 0.0969 - accuracy: 0.9615 - val_loss: 0.2268 - val_accuracy: 0.9476\n",
      "Epoch 47/50\n",
      "832/832 [==============================] - 935s 1s/step - loss: 0.1025 - accuracy: 0.9588 - val_loss: 0.1948 - val_accuracy: 0.9518\n",
      "Epoch 48/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.0923 - accuracy: 0.9650 - val_loss: 0.2038 - val_accuracy: 0.9406\n",
      "Epoch 49/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.0994 - accuracy: 0.9626 - val_loss: 0.2390 - val_accuracy: 0.9524\n",
      "Epoch 50/50\n",
      "832/832 [==============================] - 936s 1s/step - loss: 0.0904 - accuracy: 0.9642 - val_loss: 0.2741 - val_accuracy: 0.9476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7e3f7c5b2990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d42e6d7f9787413a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:55:19.107851Z",
     "start_time": "2024-06-12T20:53:50.636576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/234 [==============================] - 76s 322ms/step - loss: 0.4301 - accuracy: 0.9155\n",
      "Test Loss: 0.4301\n",
      "Test Accuracy: 91.55%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(tf_test_dataset)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c3503e739f8a60f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:55:41.252188Z",
     "start_time": "2024-06-12T20:55:19.110186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "SAVED_DIR = \"tf_model\"\n",
    "model.save_pretrained(SAVED_DIR, saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90fbb2a2-cdac-4bb3-a4f1-089edd1686d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in HF format\n",
    "model.save_pretrained(\"hf_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
