{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:23:55.807461Z",
     "start_time": "2024-06-12T12:23:52.487416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import zipfile, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import create_optimizer\n",
    "from transformers import TFAutoModelForImageClassification"
   ],
   "id": "995f8623392be588",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:00.293858Z",
     "start_time": "2024-06-12T12:24:00.216996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True) #limits gpu memory"
   ],
   "id": "7897a1a303bb1737",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T07:18:11.649216Z",
     "start_time": "2024-06-12T06:30:21.010140Z"
    }
   },
   "source": [
    "!gdown 14m2XW31x_UWAqUeoM0SNVsH3kwyBIAVy\n",
    "\n",
    "zip_ref = zipfile.ZipFile('data.zip', 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\r\n",
      "From (original): https://drive.google.com/uc?id=14m2XW31x_UWAqUeoM0SNVsH3kwyBIAVy\r\n",
      "From (redirected): https://drive.google.com/uc?id=14m2XW31x_UWAqUeoM0SNVsH3kwyBIAVy&confirm=t&uuid=743331cf-719b-4f07-bc73-d29eb1164af5\r\n",
      "To: /home/remunata/dev/bangkit/ML-Progress/Vision Transformer (ViT)/data.zip\r\n",
      "100%|███████████████████████████████████████| 2.48G/2.48G [47:36<00:00, 870kB/s]\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:13.709644Z",
     "start_time": "2024-06-12T12:24:05.097081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"imagefolder\", data_dir='data')\n",
    "\n",
    "labels = dataset['train'].features['label'].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ],
   "id": "f7966f399db61dbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/6650 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e5c610b1d6149ce9f024af7aba9e042"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/1869 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a82e05445834c66b0d6b3d06df8f7c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/1870 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adf725cb66a049cda17242108eea31e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:14.738575Z",
     "start_time": "2024-06-12T12:24:14.723687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ],
   "id": "48fad453c7b95a28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/remunata/miniconda3/envs/tfstable/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:18.914411Z",
     "start_time": "2024-06-12T12:24:18.780458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "\n",
    "train_data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.RandomCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 255.0),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")\n",
    "\n",
    "val_data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.CenterCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 255.0),\n",
    "    ],\n",
    "    name=\"val_data_augmentation\",\n",
    ")"
   ],
   "id": "e6171871a434727d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:19.359924Z",
     "start_time": "2024-06-12T12:24:19.349678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_to_tf_tensor(image: Image):\n",
    "    np_image = np.array(image)\n",
    "    tf_image = tf.convert_to_tensor(np_image)\n",
    "    return tf.expand_dims(tf_image, 0)\n",
    "\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    images = [\n",
    "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    images = [\n",
    "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ],
   "id": "b565fb2597aea54e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:21.723338Z",
     "start_time": "2024-06-12T12:24:21.659301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset[\"train\"].set_transform(preprocess_train)\n",
    "dataset[\"validation\"].set_transform(preprocess_val)\n",
    "dataset[\"test\"].set_transform(preprocess_val)"
   ],
   "id": "79033b17cad43014",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:22.226120Z",
     "start_time": "2024-06-12T12:24:22.216805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "num_train_steps = len(dataset[\"train\"]) * num_epochs\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01"
   ],
   "id": "166734ec204f0f6d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:27.136502Z",
     "start_time": "2024-06-12T12:24:23.238585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = dataset[\"train\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_eval_dataset = dataset[\"validation\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_test_dataset = dataset[\"test\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ],
   "id": "ced3c94b9eb4ab9d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:29.293461Z",
     "start_time": "2024-06-12T12:24:28.313740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ],
   "id": "a8530b99abbbf8ac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFViTForImageClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T12:24:31.112063Z",
     "start_time": "2024-06-12T12:24:31.087325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ],
   "id": "d4e91490ccfa95a9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:53:50.631966Z",
     "start_time": "2024-06-12T12:24:31.329704Z"
    }
   },
   "cell_type": "code",
   "source": "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs)",
   "id": "d99d3b812dae0902",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "832/832 [==============================] - 1026s 1s/step - loss: 0.8341 - accuracy: 0.7830 - val_loss: 0.4042 - val_accuracy: 0.9037\n",
      "Epoch 2/30\n",
      "832/832 [==============================] - 1011s 1s/step - loss: 0.3333 - accuracy: 0.8988 - val_loss: 0.6026 - val_accuracy: 0.8036\n",
      "Epoch 3/30\n",
      "832/832 [==============================] - 1016s 1s/step - loss: 0.2517 - accuracy: 0.9221 - val_loss: 0.3563 - val_accuracy: 0.9053\n",
      "Epoch 4/30\n",
      "832/832 [==============================] - 1018s 1s/step - loss: 0.2359 - accuracy: 0.9215 - val_loss: 0.3266 - val_accuracy: 0.9085\n",
      "Epoch 5/30\n",
      "832/832 [==============================] - 1016s 1s/step - loss: 0.2137 - accuracy: 0.9268 - val_loss: 0.4562 - val_accuracy: 0.8737\n",
      "Epoch 6/30\n",
      "832/832 [==============================] - 1013s 1s/step - loss: 0.1842 - accuracy: 0.9388 - val_loss: 0.3445 - val_accuracy: 0.8973\n",
      "Epoch 7/30\n",
      "832/832 [==============================] - 1013s 1s/step - loss: 0.1777 - accuracy: 0.9380 - val_loss: 0.3774 - val_accuracy: 0.9042\n",
      "Epoch 8/30\n",
      "832/832 [==============================] - 1013s 1s/step - loss: 0.1656 - accuracy: 0.9417 - val_loss: 0.3606 - val_accuracy: 0.9016\n",
      "Epoch 9/30\n",
      "832/832 [==============================] - 1017s 1s/step - loss: 0.1655 - accuracy: 0.9429 - val_loss: 0.3379 - val_accuracy: 0.9133\n",
      "Epoch 10/30\n",
      "832/832 [==============================] - 1017s 1s/step - loss: 0.1570 - accuracy: 0.9447 - val_loss: 0.4356 - val_accuracy: 0.8850\n",
      "Epoch 11/30\n",
      "832/832 [==============================] - 1018s 1s/step - loss: 0.1715 - accuracy: 0.9371 - val_loss: 0.2206 - val_accuracy: 0.9374\n",
      "Epoch 12/30\n",
      "832/832 [==============================] - 1018s 1s/step - loss: 0.1508 - accuracy: 0.9463 - val_loss: 0.2308 - val_accuracy: 0.9369\n",
      "Epoch 13/30\n",
      "832/832 [==============================] - 1025s 1s/step - loss: 0.1391 - accuracy: 0.9513 - val_loss: 0.4570 - val_accuracy: 0.8951\n",
      "Epoch 14/30\n",
      "832/832 [==============================] - 1021s 1s/step - loss: 0.1552 - accuracy: 0.9411 - val_loss: 0.2731 - val_accuracy: 0.9310\n",
      "Epoch 15/30\n",
      "832/832 [==============================] - 1021s 1s/step - loss: 0.1402 - accuracy: 0.9516 - val_loss: 0.4506 - val_accuracy: 0.8812\n",
      "Epoch 16/30\n",
      "832/832 [==============================] - 1020s 1s/step - loss: 0.1361 - accuracy: 0.9502 - val_loss: 0.1831 - val_accuracy: 0.9492\n",
      "Epoch 17/30\n",
      "832/832 [==============================] - 1018s 1s/step - loss: 0.1334 - accuracy: 0.9504 - val_loss: 0.3767 - val_accuracy: 0.9133\n",
      "Epoch 18/30\n",
      "832/832 [==============================] - 1022s 1s/step - loss: 0.1296 - accuracy: 0.9522 - val_loss: 0.2380 - val_accuracy: 0.9374\n",
      "Epoch 19/30\n",
      "832/832 [==============================] - 1018s 1s/step - loss: 0.1337 - accuracy: 0.9501 - val_loss: 0.2884 - val_accuracy: 0.9337\n",
      "Epoch 20/30\n",
      "832/832 [==============================] - 1019s 1s/step - loss: 0.1310 - accuracy: 0.9516 - val_loss: 0.3677 - val_accuracy: 0.9139\n",
      "Epoch 21/30\n",
      "832/832 [==============================] - 1021s 1s/step - loss: 0.1228 - accuracy: 0.9570 - val_loss: 0.2708 - val_accuracy: 0.9406\n",
      "Epoch 22/30\n",
      "832/832 [==============================] - 1020s 1s/step - loss: 0.1269 - accuracy: 0.9516 - val_loss: 0.2610 - val_accuracy: 0.9363\n",
      "Epoch 23/30\n",
      "832/832 [==============================] - 1021s 1s/step - loss: 0.1296 - accuracy: 0.9523 - val_loss: 0.2131 - val_accuracy: 0.9476\n",
      "Epoch 24/30\n",
      "832/832 [==============================] - 1023s 1s/step - loss: 0.1231 - accuracy: 0.9540 - val_loss: 0.3304 - val_accuracy: 0.9117\n",
      "Epoch 25/30\n",
      "832/832 [==============================] - 1021s 1s/step - loss: 0.1124 - accuracy: 0.9574 - val_loss: 0.2712 - val_accuracy: 0.9395\n",
      "Epoch 26/30\n",
      "832/832 [==============================] - 1019s 1s/step - loss: 0.1225 - accuracy: 0.9541 - val_loss: 0.2417 - val_accuracy: 0.9428\n",
      "Epoch 27/30\n",
      "832/832 [==============================] - 1017s 1s/step - loss: 0.1103 - accuracy: 0.9579 - val_loss: 0.2313 - val_accuracy: 0.9454\n",
      "Epoch 28/30\n",
      "832/832 [==============================] - 1019s 1s/step - loss: 0.1186 - accuracy: 0.9567 - val_loss: 0.2848 - val_accuracy: 0.9304\n",
      "Epoch 29/30\n",
      "832/832 [==============================] - 1018s 1s/step - loss: 0.1022 - accuracy: 0.9617 - val_loss: 0.3569 - val_accuracy: 0.9230\n",
      "Epoch 30/30\n",
      "832/832 [==============================] - 1017s 1s/step - loss: 0.1158 - accuracy: 0.9562 - val_loss: 0.3141 - val_accuracy: 0.9288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fe8b6965850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:55:19.107851Z",
     "start_time": "2024-06-12T20:53:50.636576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(tf_test_dataset)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "id": "d42e6d7f9787413a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234/234 [==============================] - 88s 377ms/step - loss: 0.4102 - accuracy: 0.9048\n",
      "Test Loss: 0.4102\n",
      "Test Accuracy: 90.48%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T20:55:41.252188Z",
     "start_time": "2024-06-12T20:55:19.110186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "version = '1'\n",
    "export_path = os.path.join(\"saved_model\", version)\n",
    "\n",
    "model.save(export_path, save_format='tf')"
   ],
   "id": "5c3503e739f8a60f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/1/assets\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
